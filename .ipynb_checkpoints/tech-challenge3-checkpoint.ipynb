{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook: Tech Challenge - Fine-Tuning a Foundation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Este notebook implementa o Tech Challenge da fase, que consiste em realizar o fine-tuning de um modelo foundational (neste caso, BERT) utilizando o dataset \"AmazonTitles-1.3MM\". O objetivo é treinar o modelo para responder perguntas dos usuários com base nos títulos e descrições de produtos do dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotecas Necessárias\n",
    "### Primeiro, instalamos e importamos as bibliotecas que serão usadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets pandas torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Escolha e Download do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "\n",
    "# O dataset \"AmazonTitles-1.3MM\" contém consultas textuais de usuários e títulos/descrições de produtos da Amazon.\n",
    "# Vamos carregar o arquivo `trn.json` conforme especificado.\n",
    "\n",
    "# Carregar o arquivo JSON\n",
    "dataset_path = \"LF-Amazon-1.3M/trn.json\"  # Substitua pelo caminho correto\n",
    "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "# Converter para DataFrame para facilitar manipulação\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Filtrar colunas relevantes: \"title\" e \"content\" (descrição)\n",
    "df = df[['title', 'content']]\n",
    "print(\"Amostra do dataset:\")\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparação do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos limpar os dados e preparar os prompts para o fine-tuning. Para o modelo de Question Answering (QA), \n",
    "# precisamos de um formato com contexto (descrição), pergunta e resposta.\n",
    "# vamos simular perguntas baseadas no título e usar a descrição como contexto.\n",
    "\n",
    "# Exemplo de função para criar perguntas simuladas\n",
    "def create_qa_pairs(row):\n",
    "    question = f\"What is the description of the product '{row['title']}'?\"\n",
    "    answer = row['content'][:512]  # Limitar a 512 caracteres para BERT\n",
    "    return {'question': question, 'context': row['content'], 'answer': answer}\n",
    "\n",
    "# Aplicar a função ao dataset\n",
    "qa_data = df.apply(create_qa_pairs, axis=1).tolist()\n",
    "\n",
    "# Converter para o formato esperado pelo Hugging Face Datasets\n",
    "qa_dataset = Dataset.from_list(qa_data)\n",
    "print(\"Amostra do dataset preparado:\")\n",
    "print(qa_dataset[0])\n",
    "\n",
    "# Dividir em treino e validação\n",
    "qa_dataset = qa_dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chamada do Foundation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos usar o BERT para Question Answering. Primeiro, testamos o modelo pré-treinado antes do fine-tuning.\n",
    "\n",
    "# Carregar o tokenizador e o modelo\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Função para tokenizar os dados\n",
    "def preprocess_function(examples):\n",
    "    encodings = tokenizer(\n",
    "        examples['question'],\n",
    "        examples['context'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=512,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    # Para simplificar, assumimos que a resposta está no início do contexto (ajuste conforme necessário)\n",
    "    encodings['start_positions'] = [0] * len(examples['answer'])\n",
    "    encodings['end_positions'] = [min(len(answer.split()), 512) for answer in examples['answer']]\n",
    "    return encodings\n",
    "\n",
    "# Aplicar pré-processamento\n",
    "tokenized_dataset = qa_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Teste antes do fine-tuning\n",
    "sample = tokenized_dataset['train'][0]\n",
    "inputs = {key: torch.tensor(val).unsqueeze(0) for key, val in sample.items() if key in ['input_ids', 'attention_mask', 'token_type_ids']}\n",
    "outputs = model(**inputs)\n",
    "print(\"Exemplo de saída antes do fine-tuning:\")\n",
    "print(tokenizer.decode(outputs.start_logits.argmax().item()), tokenizer.decode(outputs.end_logits.argmax().item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Execução do Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora, realizamos o fine-tuning do modelo com o dataset preparado.\n",
    "\n",
    "# Configurar argumentos de treinamento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Inicializar o Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    ")\n",
    "\n",
    "# Treinar o modelo\n",
    "trainer.train()\n",
    "\n",
    "# Salvar o modelo treinado\n",
    "model.save_pretrained('./fine_tuned_bert')\n",
    "tokenizer.save_pretrained('./fine_tuned_bert')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Geração de Respostas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuramos o modelo treinado para responder perguntas dos usuários.\n",
    "\n",
    "# Função para gerar respostas\n",
    "def generate_answer(question, context):\n",
    "    inputs = tokenizer(question, context, return_tensors='pt', truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    start_idx = outputs.start_logits.argmax()\n",
    "    end_idx = outputs.end_logits.argmax()\n",
    "    answer = tokenizer.decode(inputs['input_ids'][0][start_idx:end_idx+1])\n",
    "    return answer\n",
    "\n",
    "# Exemplo de uso\n",
    "sample_question = \"What is the description of the product 'Wireless Mouse'?\"\n",
    "sample_context = df['content'].iloc[0]  # Usar uma descrição do dataset\n",
    "response = generate_answer(sample_question, sample_context)\n",
    "print(f\"Pergunta: {sample_question}\")\n",
    "print(f\"Resposta: {response}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Este notebook detalha o processo de:\n",
    "# 1. Seleção e preparação do dataset AmazonTitles-1.3MM.\n",
    "# 2. Fine-tuning do modelo BERT para QA.\n",
    "# 3. Geração de respostas a partir de perguntas dos usuários.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
